{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zcwisc/GB657/blob/main/Module_1_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: Clustering\n",
        "\n",
        "As mentioned, there are (at least) two basic machine learning setups.  In **supervised** machine learning one observes a response $Y$ with observing $p$ different features $X=(X_1,X_2,\\ldots,X_p)$, where we typically postulate the relationship $Y = f(X)+\\varepsilon$ and $\\varepsilon$ independent of $X$ with mean zero. Here quality is usually assessed by the (test/out-of-sample) error that compares predictions and realizations for a separate dataset.  \n",
        "\n",
        "In **unsupervised** learning, we only observe $p$ features $X_1,X_2,\\ldots,X_p$, and we would like to learn about their relationship -- without focussing on a supervising outcome.  Of course, the difficulty is how to assess quality in this case -- so different unsupervised learning techniques are very different, and which one to pick will depend on the nature of the problem.\n",
        "\n",
        "In this tutorial, we will take a closer look at one important class of unsupervised learning algorithms: **Cluster Analysis**.  We start with a quick introduction but then look into the applications of these techniques in more detail in the context of a case study.\n",
        "\n",
        "As usual, we start by implementing the relevant packages:"
      ],
      "metadata": {
        "id": "aIiejbOjljzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import plotly.figure_factory as ff\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from random import sample\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler # For rescaling metrics to fit 0 to 1 range\n",
        "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import euclidean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering Analysis Background\n",
        "\n",
        "### Background\n",
        "\n",
        "*Clustering* refers to techniques for finding subgroups in a given dataset. The typical approach to determine clusters $C_1,\\ldots,C_K$ is to minimize:\n",
        "$$\n",
        "\\sum_{k=1}^K W(C_k),\n",
        "$$\n",
        "where $W$ is a measure of *variation* within a cluster.  For instance, **k-means clustering** uses the Euclidean distance to measure variation:\n",
        "$$\n",
        "W(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2.\n",
        "$$\n",
        "The algorithms are implemented via a greedy algorithm by considering the centers of clusters (referred to as *centroids*).  The number of clusters $K$ must be chosen beforehand.  One approach is *hierarchical clustering*, where one starts with a larger number of clusters and then *fuses* custers that are similar (e.g., with regards to the distance between their centroids).\n",
        "\n",
        "### Simulated Example\n",
        "\n",
        "Let's consider a very basic simulated example -- let's simulate normal random variables with different means:"
      ],
      "metadata": {
        "id": "OEru_Pl4ACGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw = np.random.multivariate_normal((0,0), np.array([[1, 0], [0, 1]]), size=100)\n",
        "X_raw[0:49,0]=X_raw[1:50,0]+3\n",
        "X_raw[0:49,1]=X_raw[1:50,1]-4"
      ],
      "metadata": {
        "id": "tCFUcnLqbdpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot:"
      ],
      "metadata": {
        "id": "Ozl7WSIvbdMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[0:49,0], X_raw[0:49,1], color='red')\n",
        "plt.scatter(X_raw[50:99,0], X_raw[50:99,1], color='black')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AG2cFW5VctpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's run k means clustering with two clusters (notice that we are setting a random state so we can replicate the process):\n"
      ],
      "metadata": {
        "id": "5WdTAU69d6rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "kmeans.fit(X_raw)\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroids"
      ],
      "metadata": {
        "id": "oq_YzcFwd6dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's include the centroids in the picture:"
      ],
      "metadata": {
        "id": "NVCI1UzeDFR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[0:49,0], X_raw[0:49,1], color='red', label='Cluster 1')\n",
        "plt.scatter(X_raw[50:99,0], X_raw[50:99,1], color='black', label='Cluster 2')\n",
        "plt.scatter(centroids[:,0], centroids[:,1], color='blue', marker='X', s=200, label='Centroids') # Plot centroids\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-gHgWtJ_Cvcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's check the cluster allocation:"
      ],
      "metadata": {
        "id": "MHFfqMuKDI5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label = kmeans.fit_predict(X_raw)\n",
        "label"
      ],
      "metadata": {
        "id": "PhY9123eecf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[:, 0], X_raw[:, 1], c=label, cmap='viridis')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.title('K-Means Clustering Results (Simulated Data)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lFauyhimCsHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the algorithm was able to identify how we set up the data!"
      ],
      "metadata": {
        "id": "TPcmD4aOeq3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cluster Analysis Case Study: County Health Rankings 2013\n",
        "\n",
        "We analyze County Health Rankings in the US in 2013, based on a data set from the University of Wisconsin Population Health Institute.\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "3RW20yqheke7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge_n_HRNgRm0"
      },
      "source": [
        "!git clone https://github.com/zcwisc/GB657"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whMtWdEgtAh"
      },
      "source": [
        "health = pd.read_csv('GB657/Module_1_CountyHealthR.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "V3w077i9fShr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's a large dataset, and the first three numbers are indices.  However, the remaning columns provide rather detailed information for each county.  \n",
        "\n",
        "Let's drop the features with many missing values and the index information. Afterwards, let's drop the counties with missing features."
      ],
      "metadata": {
        "id": "W6DWnaeb0RHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health = health.drop(columns=['FIPS','State','County','Perc.Fair.Poor.Health','Perc.Smokers','Perc.Excessive.Drinking','MV.Mortality.Rate','Pr.Care.Physician.Ratio','Perc.No.Soc.Emo.Support'])\n",
        "health = health.dropna()"
      ],
      "metadata": {
        "id": "-lHmYJjIEhkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the resulting data:"
      ],
      "metadata": {
        "id": "AOvjhvUr1Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "YHiF5HDW1PnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.describe()"
      ],
      "metadata": {
        "id": "oeIESh-O1SwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize:"
      ],
      "metadata": {
        "id": "N_rCrzibFIWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(health.corr());"
      ],
      "metadata": {
        "id": "z8Ia-qwL0k1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So quite a bit of correlation, but nothing seems to be \"replicated.\"\n",
        "\n",
        "And let's scale the data so as to make sure we are comparing apples to apples:"
      ],
      "metadata": {
        "id": "T2-2bu_51cH1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVSWGGQ-kVhg"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(health)\n",
        "health_sc = scaler.transform(health)\n",
        "health_sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means clustering\n",
        "\n",
        "Let's commence with k-means clustering, which is avalable in the library `sklearn.cluster` (along with other clustering algorithms, see below). To decide on the number of clusters, we evaluate how the within-sum-of-squares varies between the number of clusters using a so-called *elbow plot*:\n"
      ],
      "metadata": {
        "id": "VGmhd4rb1dlV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF4Tvu-imXH7"
      },
      "source": [
        "wcss = []\n",
        "for i in range(2, 12):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "    kmeans.fit(health_sc)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.bar(range(2, 12), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that six clusters seem to present a reasonable choice, there is a bit of an \"elbow\" there. So let's go with six:"
      ],
      "metadata": {
        "id": "LlIwaq1N3J4j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1JGhuVnqGv"
      },
      "source": [
        "kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "kmeans.fit(health_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the labels via:"
      ],
      "metadata": {
        "id": "cOXho1k23ok6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "jtsoEKP_3tLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add to our dataset:"
      ],
      "metadata": {
        "id": "fgE-k3VH9nLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "GJ_sfTvh9pE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the different clusters by evaluating the YPLL.Rate (Years of Potential Life Lost)."
      ],
      "metadata": {
        "id": "hHZYmnlK_Jrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bG0zgwt5-XMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "HTplXWaf_eLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ddf2dbEl_eON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "QZPQh5Xc_eRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "kq_DhtSp_eTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "pNqE83nb_n-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's relabel so it's increasing:"
      ],
      "metadata": {
        "id": "euotX4dR_tFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relab(label):\n",
        "  if label == 0:\n",
        "    return 0\n",
        "  elif label == 1:\n",
        "    return 1\n",
        "  elif label == 2:\n",
        "    return 5\n",
        "  elif label == 3:\n",
        "    return 4\n",
        "  elif label == 4:\n",
        "    return 3\n",
        "  else:\n",
        "    return 2"
      ],
      "metadata": {
        "id": "Umgy4pQ-F7m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmlab = list(map(relab, kmeans.labels_))"
      ],
      "metadata": {
        "id": "reVaZbJTHH08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmlab"
      ],
      "metadata": {
        "id": "LeY28w_K5hGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering\n",
        "\n",
        "Let's use the hierarchical clustering algorithm in scikit (`AgglomerativeClustering`). Plotting a *dendrogram* is easier with the `scipy.cluster.hierarchy`, yet given the size of the dataset we don't see a ton here.\n",
        "\n",
        "To compare with the k-means approach, let's run a hierarchical clustering also with six clusters."
      ],
      "metadata": {
        "id": "yWrU6Bnl3u0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch = AgglomerativeClustering(n_clusters=6)\n",
        "hierarch.fit(health_sc)"
      ],
      "metadata": {
        "id": "1oBxQTqs7p4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can get the labels via:"
      ],
      "metadata": {
        "id": "gYDv_mNx8tTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch.labels_"
      ],
      "metadata": {
        "id": "Ogxz292p8J9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also add to the dataset and carry out a similar approach as before:"
      ],
      "metadata": {
        "id": "Q3_hVGRN55AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierarch.labels_"
      ],
      "metadata": {
        "id": "3mgy8-NK5-Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ib6Xuvyo8p4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "X9t8epBf8p1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "lpS1mFd18pyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bMakZhkB8pq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "d0sBEYLn8pgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "Inm4wef88zpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relab2(label):\n",
        "  if label == 0:\n",
        "    return 5\n",
        "  elif label == 1:\n",
        "    return 2\n",
        "  elif label == 2:\n",
        "    return 0\n",
        "  elif label == 3:\n",
        "    return 4\n",
        "  elif label == 4:\n",
        "    return 3\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "5cHreh5l9BK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierlab = list(map(relab2, hierarch.labels_))"
      ],
      "metadata": {
        "id": "6rUa25gq9X_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierlab"
      ],
      "metadata": {
        "id": "kgom-7Rl9hCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two:"
      ],
      "metadata": {
        "id": "VzFnwZJW5yj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0,1,2,3,4,5]\n",
        "confusion_matrix(health['k_means_cl'],health['k_hier_cl'],labels=labels)"
      ],
      "metadata": {
        "id": "u84XOdUS9qs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like there is quite a bit of overlap, particularly with regards to clusters 0, 1, 2, and 5. It appears clusters 3 and 4 are overlapping a little bit. However, it also is clear that assigning points to clusters isn't highly obvious."
      ],
      "metadata": {
        "id": "ps_F7hxy8JrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Clusters\n",
        "\n",
        "Let's visualize clusters by comparing the distributions of some of the features across clusters. As we had discussed, comparing distributions can be conveniently accomplished via box-whisker plots.\n",
        "\n",
        "Let's start with children in poverty."
      ],
      "metadata": {
        "id": "X7GF4qiTA0Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Children.in.Poverty\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HYKSMgk6_wJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we sorted the clusters according to YPLL.Rate (Years of Potential Life Lost), so more years of life lost is a sign of worse health. It appears that this weakly aligns with children poverty.\n",
        "\n",
        "Let's look at violent crimes."
      ],
      "metadata": {
        "id": "0CDlt4LMBRi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Violent.Crime.Rate\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BD-oXwLXBwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we have a weak alignment, although cluster 4 seems a bit lower. Let's look at fast food concentration."
      ],
      "metadata": {
        "id": "gypA3GKUB9JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Fast.Foods\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yRJkARhMCQDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check particulates:"
      ],
      "metadata": {
        "id": "PFp9jU8byHS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Avg.Daily.Particulates\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "deXCjOP2vk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the counties in cluster 1 and 4 seem to have more pollution.\n",
        "\n",
        "We can look more into the clusters and try to describe them. One way to think about getting a good interpretation is to think of **stereotypes** of each clusters. That is, thinking about the \"typical stick-figure\" representative of cluster representatives: Imagine the typical example of a county in this cluster.\n",
        "\n",
        "However, here, rather than looking more into the clusters, what we will do is to plot a map with US counties and color them according to their cluster allocations. The code is a little complex (I used chatpgt to generate it :-) but it works.\n",
        "\n",
        "First, let's make a dataset that has FIPS (county) codes and the cluster allocations:"
      ],
      "metadata": {
        "id": "wtlCpPE2ygO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health2 = pd.read_csv('GB657/Module_1_CountyHealthR.csv')\n",
        "fips_codes = health2['FIPS']\n",
        "cluster_labels = health['k_means_cl']"
      ],
      "metadata": {
        "id": "8EX32iMMG7sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fips_cluster_df = pd.DataFrame({'fips': fips_codes, 'cluster': cluster_labels})"
      ],
      "metadata": {
        "id": "Kuv1nsqUKb-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, then, let's plot the map:"
      ],
      "metadata": {
        "id": "-CdkSZLVPnkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "import matplotlib.colors as mcolors\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "\n",
        "# Step 1: Download TIGER/Line U.S. counties shapefile (using 2023 data)\n",
        "url = \"https://www2.census.gov/geo/tiger/TIGER2023/COUNTY/tl_2023_us_county.zip\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Extract the zipfile into memory\n",
        "with ZipFile(BytesIO(response.content)) as zf:\n",
        "    zf.extractall(\"tl_2023_us_county\")  # Extract to a folder\n",
        "\n",
        "# Load the shapefile\n",
        "counties = gpd.read_file(\"tl_2023_us_county/tl_2023_us_county.shp\")\n",
        "\n",
        "# Ensure FIPS codes are strings and zero-padded to 5 characters\n",
        "fips_cluster_df['fips'] = fips_cluster_df['fips'].astype(str).str.zfill(5)\n",
        "\n",
        "# Step 2: Merge the shapefile data with the cluster data\n",
        "counties = counties.merge(fips_cluster_df, left_on='GEOID', right_on='fips', how='left')\n",
        "\n",
        "# Step 3: Define a custom colormap with only as many colors as clusters\n",
        "unique_clusters = counties['cluster'].dropna().unique()\n",
        "num_clusters = len(unique_clusters)\n",
        "\n",
        "# Use a discrete colormap for clarity\n",
        "# cmap = plt.cm.get_cmap('tab20', num_clusters)  # Get only `num_clusters` colors\n",
        "# norm = mcolors.BoundaryNorm(range(num_clusters + 1), cmap.N)\n",
        "\n",
        "# Step 4: Plot the map with a larger size\n",
        "fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
        "\n",
        "# Create discrete colormap (viridis style, but one color per cluster)\n",
        "unique_clusters = sorted(counties['cluster'].dropna().unique())\n",
        "num_clusters = len(unique_clusters)\n",
        "cmap = plt.cm.get_cmap('viridis', num_clusters)\n",
        "norm = mcolors.BoundaryNorm(np.arange(num_clusters + 1) - 0.5, num_clusters)\n",
        "\n",
        "# Plot without GeoPandas legend\n",
        "counties.plot(column='cluster', cmap=cmap, ax=ax, legend=False,\n",
        "              norm=norm, missing_kwds={\"color\": \"lightgrey\", \"label\": \"No Data\"})\n",
        "\n",
        "# Add compact colorbar with discrete ticks\n",
        "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "sm.set_array([])\n",
        "cbar = plt.colorbar(sm, ax=ax, fraction=0.03, pad=0.02,\n",
        "                    ticks=np.arange(num_clusters))\n",
        "cbar.ax.tick_params(labelsize=8)\n",
        "cbar.set_label(\"Cluster\", fontsize=10)\n",
        "\n",
        "# Zoom and clean up\n",
        "ax.set_xlim(-125, -66.5)\n",
        "ax.set_ylim(24, 50)\n",
        "ax.set_title('US Counties Colored by Cluster Allocation', fontsize=20)\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"us_county_clusters.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XLJ7G3kON-15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Clearly, there is a geographical component to health (maybe due to pollution). But there also appears to be a political-economic dimension. This illustrates what clustering can achieve! (It may be hard to see, so check out the saved png file.)"
      ],
      "metadata": {
        "id": "O_D9oN-uPzPn"
      }
    }
  ]
}